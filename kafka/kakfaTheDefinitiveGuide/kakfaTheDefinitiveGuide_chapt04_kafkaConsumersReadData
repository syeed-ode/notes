Add 23 to the page to get the pdf location. A page references are in PDF pages.
*************************************************************
*							 *
*							 *
*							 *
toc: 20
pg43 - 
*************************************************************
*** Chapter 4 - Kafka Consumers: Reading Data from Kafka ****
*************************************************************	
Kafka Definitive Guide
Kafka Consumer Concepts - 63 (86 of 322) 
	Consumers and Consumer Groups
			Suppose you have an application that needs to 
					read messages from a Kafka topic, 

					run some validations against them, and 

					write the results to another data store. 

			In this case your application will 
					create a consumer object, 

					subscribe to the appropriate topic, and 

					start receiving messages, 

					validating them and 

					writing the results. 

					This may work well for a while, but what if the rate at which producers write messages to the topic exceeds the rate at which your application can validate them? 

					Obviously there is a need to scale consumption from topics.

					Just like multiple producers can write to the same topic, 

			We need to allow multiple consumers to read from the same topic, splitting the data between them.

			Kafka consumers are typically part of a consumer group.
					When multiple consumers are subscribed to a topic and belong to the same consumer group, 

			Each consumer in the group will receive messages from a different subset of the partitions in the topic.
					Let’s take topic T1 with four partitions. Now suppose we created a new consumer, C1,which is the only consumer in group G1, 
							Consumer C1 will get all messages from all four t1 partitions.

					If we add another consumer, C2, to group G1, each consumer will only get messagesfrom two partitions. 

					If G1 has four consumers, then each will read messages from a single partition.

					If we add more consumers to a single group with a single topic than we have parti‐tions, some of the consumers will be idle and get no messages at all.

			The main way we scale data consumption from a Kafka topic is by adding more consumers to a consumer group.
					It is common for Kafka consumers to do 
							high-latency operations such as write to a database or a 

							time-consuming computation on the data.

			This is a good reason to create topics with a large number of partitions:
					our main method of scaling is

					adding more consumers that share the load by 

					having each consumer own just a subset of the partitions and messages

					it allows adding moreconsumers when the load increases.

			Keep in mind that there is no point in adding more consumers than you have partitions in a topic
					some of the consumers will just be idle

					In addition to adding consumers in order to scale a single application, 
			
			It is very common to have multiple applications that need to read data from the same topic.
					One of the main design goals in Kafka was to make the data produced to Kafka topics
							available for many use cases throughout the organization. In those cases, 

							we want each application to get all of the messages rather than just a subset

			Ensure the application has its own consumer group, 
					to make sure anapplication gets all the messages in a topic.

					Kafka scales to a large number of consumers and consumer groups without reducing performance.

					if we add a new consumer group G2 with a single consumer,
							this consumer will get all the messages in topic T1 independent of what G1 is doing

							G2 can have more than a single consumer, in which case they will each get a subset of partitions,

							but G2 as a whole will still get all the messages regardless of other consumer groups.

			To summarize,
					you create a new consumer group for each application that needs allthe messages from one or more topics

					you add consumers to an existing consumergroup to scale the reading and processing of messages from the topics
							so each addi‐tional consumer in a group will only get a subset of the messages.

	Consumer Groups and Partition Rebalance
			Consumers in a consumer group share ownershipof the partitions in the topics they subscribe to
					When we add a new consumer to the group, it starts consuming messages from partitions previously consumed by another consumer. 

					The same thing happens when a consumer shuts down or crashes; it leaves the group, and 

			when a consumer shuts down the partitions it used to consume will be consumed by one of the remaining consumers

			Reassignment of partitions to consumers also happen whenthe topics the consumer group is consuming are modified
					if an administrator adds new partitions

			Rebalancing
					Moving partition ownership from one consumer to another

					Rebalances allowing us to easily and safely add and remove consumers
							in the normal course of events they are fairly undesirable. 
									During a rebalance, consumers can’t consume messages 

					so a rebalance is basically a short window of unavailability of the entire consumer group.
							when partitions are moved from one consumer to another, the consumer loses its current state;
									if it was caching any data, it will need to refresh its caches

									slowing down the application until the consumer sets up its state again.

					Throughout this chapter we will discuss how to safely handle rebalances and how to avoid unnecessary ones

			Heartbeats (skipping for right now) - pg 67 (90 of 322)

			How Does the Process of Assigning Partitions to Brokers Work?
					When a consumer wants to join a group, it sends a JoinGroup request to the group coordinator.

					The first consumer to join the group becomes the group leader.
							The leader receives a list of all consumers in the group from the group coordinator 
									(this willinclude all consumers that sent a heartbeat recently and 

									which are therefore considered alive) and 

							The leader is responsible for assigning a subset of partitions to each consumer.
									It uses an implementation of PartitionAssignor to decide which partitions should be handled by which consumer.


							The consumer leader sends the list of assignments to the GroupCoordinator
									The GroupCoordinator sends this information to all the consumers

									the leader is the only client process that has 
											the full list of consumers in the group and 

											their assignments

											Each consumer only sees his own assignment

Creating a Kafka Consumer
	(skipping the notes for now...need to expidite the process)



The Poll Loop
	At the heart of the consumer API is a simple loop for polling the server for more data.

	The poll loop handles all details of coordina‐tion, partition rebalances, heartbeats, and data fetching
			Once the consumer subscribes to topics,

			This leaves the developer with aclean API that simply returns available data from the assigned partitions.
							 public KafkaConsumer<String, String> createKafkaConsumer() {
							  Properties props = new Properties();
							  props.put("bootstrap.servers", "broker1:9092,broker2:9092");
							  props.put("group.id", "CountryCounter");
							  props.put("key.deserializer"
							  		, "org.apache.kafka.common.serialization.StringDeserializer");
							  props.put("value.deserializer"
							  		, "org.apache.kafka.common.serialization.StringDeserializer");

							  return new KafkaConsumer<String, String>(props);
							 }


							 public void mainBodyOfConsumer(KafkaConsumer<String, String> consumer) {
							  Map<String, Integer> custCountryMap = new HashMap();
							  try {
							   while (true) {																	// 1
							    ConsumerRecords<String, String> records = consumer.poll(100);				// 2
							    for (ConsumerRecord<String, String> record : records) {						// 3
							     log.debug("topic = %s, partition = %s
							     	, offset = %d, customer = %s
							     	, country = %s\n"
							       , record.topic()
							       , record.partition()
							       , record.offset()
							       , record.key()
							       , record.value());
							     int updatedCount = 1;
							     if (custCountryMap.containsValue(record.value())) {
							      updatedCount = custCountryMap.get(record.value()) + 1;
							     }
							     custCountryMap.put(record.value(), updatedCount);
							     JSONObject json = (JSONObject) 
							     	new HashMap<String, Integer>(custCountryMap);
							     System.out.println(json.toString());									// 4
							    }
							   }
							  } finally {
							   consumer.close();																// 5
							  }
							 }

					 // 1
					 		This is indeed an infinite loop. Consumers are usually long-running applicationsthat continuously poll Kafka for more data. 
					 				We will show later in the chapter how to cleanly exit the loop and close the consumer.

	 				// 2
	 						*****The same way that sharks must keep moving or they die, consumers must keep polling Kafka or they will be considered dead****** and 
	 								the partitions they are consuming will be handed to anotherconsumer in the group to continue consuming. 

								The parameter we pass, poll(), is a timeout interval 
										it controls how long poll() will block if data is not available in the consumer buffer. 
												If this is set to 0, poll() will return immediately;

												otherwise, it will wait for the specified number of milliseconds for data to arrivefrom the broker.

						// 3
								poll() returns a list of records.
										Each record contains the topic and partition the record came from
												, the offset of the record within the partition

												, and of course the key and the value of the record. 

										Typically we want to iterate over the list and pro‐cess the records individually.

								The poll() method takes a timeout parameter. 
										This specifies how long it will take poll to return, with or without data. 

										The value is driven by application needs for quick responses -- how fast do you want to return control to the thread that does the polling?

						// 4
								Processing usually ends in writing a result in a data store or updating a stored record. 
										Here, the goal is to keep a running count of customers from each county,so we update a hashtable and print the result as JSON.

										A more realistic examplewould store the updates result in a data store.

						// 5
								Always close() the consumer before exiting.
										This will close the network connec‐tions and sockets.

										It will also trigger a rebalance immediately rather than wait for the group coordinator to discover that the consumer stopped sending heartbeats
												and is likely dead, which will take longer and 

												therefore result in a longer period of time in which 
														consumers can’t consume messages from a subset of the partitions.

			The poll loop does a lot more than just get data.
					The first time you call poll() with a new consumer,

					, it is responsible for finding the GroupCoordinator

					, joining the consumer group and

					, receiving a partition assignment.

					If a rebalance is triggered, it will be handled inside the poll loop as well. 

					And of course the heartbeats that keep con‐sumers alive are sent from within the poll loop.

			For this reason, we try to make sure that whatever processing we do between iterations is fast and efficient.

			Thread Safety: One consumer per thread is the rule.

Configuring Consumers
	

Commits and Offsets - the position within the Kafka consumer topic's partition / We call the action of updating the current position in the partition a commit

	One  of  Kafka’s  uniquecharacteristics  is  that  it  does  not  track  acknowledgments  from  consumers  the  waymany  JMS  queues  do.

	Instead,  it  allows  consumers  to  use  Kafka  to  track  their  position (offset) in each partition.

	We call the action of updating the current position in the partition a commit.

	How does a consumer commit an offset? It produces a message to Kafka, to a special__consumer_offsets topic, with the committed offset for each partition.

	If  a  consumer  crashes  or  a  new  consumer  joins  the  consumer  group,  this  will 'trigger  a  rebalance'.
			After  a  rebalance,  each  consumer  may  be  assigned  a  new  set  ofpartitions  than  the  one  it  processed  before. 

			In  order  to  know  where  to  pick  up  thework,  the  consumer  will  read  the  latest  committed  offset  of  each  partition  and  continue from there.
					If  the  committed  offset  is  smaller  than  the  offset  of  the  last  message  the  client  processed,  
							the  messages  between  the  last  processed  offset  and  the  committed  offset 

							will be processed twice.

					If the committed offset is larger than the offset of the last message
							the  messages  between  the  last  processed  offset  and  the  committed  offset 

							will be missed by the consumer group.

Rebalance Listeners
	






‐
































































